Raft 是 nacos 集群使用的一种分布式共识算法  
可以保障分布式系统数据的一致性  
性能等同于 Paxos 共识算法  

---  

共识：分布式中多个节点对数据达成一致  
客户端一般只会和集群中的一个节点进行交互，这一个节点上的某个数据发生修改后，其他节点也需要同步修改这个数据以保证数据一致  

数据的最终一致性：无论集群是 CP 的还是 AP 的，一个节点上的数据发生修改后，最终所有节点都要达成共识  

---  

Raft 算法中，所有数据都是通过 Leader 节点进行写入的  

Leader 节点的选举机制  
- 集群各个节点启动，都是**Follower**从节点，每个节点维护一个**Term**任期 0  
- 节点启动后开启一个随机时长的定时任务（150 - 300s 之间不等）  
    - 在定时范围内如果有 **Leader** 主节点发送过来的心跳，则随机重置时长  
    - 在定时范围内如果收到其他节点发来的**投票请求**，重置时长  
        - 该候选人的 Term 不小于自己，并且自己的票还未投出去，则把票投给这个 Candidate  
        然后将 Term 更新为候选人的 Term，将自己标识为已投票的状态  
        - 投票之后并不会停止计时。如果计时结束前仍没有收到新 Leader 的消息，则触发任务  
- 触发定时任务，**Follower** 会升级为 **Candidate** 候选人，将其本地 Term 加一，向自己投一票并向集群中的其他节点发起**投票请求**
    - 如果变成 Candidate 给自己投票之前收到来自其他 Candidate 的投票请求，其 Term 不小于自己，则把票投给他并回退为 Follower  
    - 等待投票结果期间收到新 Leader 通知，并且其 Term 不小于自己，则退回 Follower  
    - 收到超过半数投票的 Candidate 会升级为 **Leader** 主节点  
    - 没有收到过半选票也没有新 Leader 通知，则重新发出选举  
    - 如果出现了多个 Candidate 同时收到了同样多的选票，则选举作废，节点各自重新创建随机时长的定时任务，以重新选举，直到出现新 Leader 为止。这个情况概率比较小  
- Leader 节点通过心跳告知 Follower 节点重置时长。一旦 Leader 不能维持这个心跳，则一定会有某个 Follower 的定时任务先被触发，重新进行选举。新 Leader 出现后所有节点 **Term+1**  
    - 旧 Leader 恢复后，会因 Term 比其他节点更低无法立即恢复 Leader 身份。Follower 不会因 Term 比自己低的节点发来的请求重置定时任务。新 Leader 向旧 Leader 发送心跳会更新其 Term 并将其设为 Follower  

---  

日志复制  
- 客户端的每个更改，主节点 Leader 都会将其作为一个条目 Entry 添加到节点日志中  
- 首先 Leader 将 Entry 复制到 Followers  
- Leader 等待，直到大多数节点都响应表示写入了 Entry，才能**提交**Entry  
- 最后 Leader 通知 Followers，该 Entry 已提交  
- 如果没有主节点，就无法完成日志复制，客户端需要一直请求获取到 Leader 节点后才能发起修改请求  

raft 如何保证数据最终一致 [see](4/2.md)  

---  

脑裂问题  
- Leader 和某个 Follower 之间断网，导致这个 Follower 升级成为 Leader，造成一个整体的集群分裂成两个甚至多个独立节点  
- 这些分裂出来的节点开始争抢共享资源造成系统混乱、数据损坏的现象称为脑裂，也成为网络分区  

Raft 算法下的集群脑裂情况 [see](4/1.md)  
Raft 如何应对脑裂  

---

源码分析  
- 投票的发起和计算 [see](image/1.png)  
- 收到投票请求后的处理逻辑 [see](image/2.png)  

[back](../15.md)  